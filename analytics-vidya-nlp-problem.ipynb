{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I want to thank [@srk](http://www.kaggle.com/sudalairajkumar) for his wonderful kernel on how to get started in NLP. Really learnt a lot and tried to apply some of them to process text data.\n\nYou can find the kernel [here](http://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing)"},{"metadata":{},"cell_type":"markdown","source":"Let us begin with importing the libraries and datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('../input/analytics-vidya-nlp-data/train_E6oV3lV.csv');\ntest = pd.read_csv('../input/analytics-vidya-nlp-data/test_tweets_anuFYb8.csv');\nss = pd.read_csv('../input/analytics-vidya-nlp-data/sample_submission_gfvA5FD.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOWERING "},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text_lower\"] = train[\"tweet\"].str.lower();\ntest[\"text_lower\"] = test[\"tweet\"].str.lower()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# REMOVING PUNCTUATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"PUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ntrain[\"text_wo_punct\"] = train[\"text_lower\"].apply(lambda text: remove_punctuation(text));\ntest[\"text_wo_punct\"] = test[\"text_lower\"].apply(lambda text: remove_punctuation(text))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STOPWORDS"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"STOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ntrain[\"text_wo_stop\"] = train[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\ntest[\"text_wo_stop\"] = test[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = train.drop(columns = ['tweet', 'text_lower', 'text_wo_stop'], axis = 1);\nte = test.drop(columns = ['tweet', 'text_lower', 'text_wo_stop'], axis = 1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STEMMING"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\n\n\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ntr[\"text_stemmed\"] = tr[\"text_wo_punct\"].apply(lambda text: stem_words(text));\nte[\"text_stemmed\"] = te[\"text_wo_punct\"].apply(lambda text: stem_words(text))\ntr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LEMMATIZING"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\ntr[\"text_lemmatized\"] = tr[\"text_stemmed\"].apply(lambda text: lemmatize_words(text));\nte[\"text_lemmatized\"] = te[\"text_stemmed\"].apply(lambda text: lemmatize_words(text))\ntr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom sklearn.impute import SimpleImputer;\nfrom sklearn.compose import ColumnTransformer;\nfrom sklearn.pipeline import Pipeline;\nfrom sklearn.preprocessing import LabelEncoder;\nfrom sklearn.preprocessing import StandardScaler;\nfrom sklearn.preprocessing import MinMaxScaler;\nfrom sklearn.model_selection import train_test_split;\nfrom sklearn.linear_model import LinearRegression ;\nfrom sklearn.linear_model import Ridge, Lasso;\nfrom sklearn.metrics import mean_squared_error;\nfrom sklearn.metrics import r2_score;\nfrom sklearn.preprocessing import PolynomialFeatures;\nfrom sklearn.svm import SVR;\nfrom sklearn.svm import SVC;\nfrom sklearn.tree import DecisionTreeClassifier;\nfrom sklearn.ensemble import RandomForestClassifier;\nfrom sklearn.ensemble import RandomForestRegressor;\nfrom sklearn.neighbors import KNeighborsClassifier;\nfrom sklearn.naive_bayes import GaussianNB;\nimport xgboost as xgb;\nfrom xgboost import XGBClassifier;\nfrom xgboost import XGBRegressor;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LABEL ENCODING AND SPLITING INTO TEST AND TRAIN DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tr.text_lemmatized.values;\ny = tr.label.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lab = LabelEncoder();\nx = lab.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z = te.text_lemmatized.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z = lab.fit_transform(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2, random_state = 55)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RANDOMFORREST"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier();\nrfc.fit(xtrain, ytrain);\nrfc.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"xx = XGBClassifier();\nxx.fit(xtrain, ytrain);\nxx.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LIGHTGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nlgb = LGBMClassifier(n_estimators=10)\nlgb.fit(xtrain,ytrain);\nlgb.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNEARESTNEIGHBOUR"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier();\nknn.fit(xtrain, ytrain);\nknn.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DECISION TREE"},{"metadata":{"trusted":true},"cell_type":"code","source":"dc = DecisionTreeClassifier();\ndc.fit(xtrain, ytrain);\ndc.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are getting best results from xgboost.Its better and safer to go with that one I guess."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}